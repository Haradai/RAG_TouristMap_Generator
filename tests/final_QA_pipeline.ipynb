{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim here is to improve the retrieving tools for the agent so that more useful information is extracted.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "-Experiment with different retrievers, more sparse like bm25\n",
    "\n",
    "-Play around with ranker models. (DiversityRanker and LostInTheMiddleRanker) and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepsmachine/miniforge3/envs/haystack_stuff/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_haystack import QdrantDocumentStore\n",
    "\n",
    "document_store = QdrantDocumentStore(\n",
    "    path=\"qdrant\",\n",
    "    index=\"Document\",\n",
    "    embedding_dim=768,\n",
    "    recreate_index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Playing with retrievers only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepsmachine/miniforge3/envs/haystack_stuff/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You seem to be using sentence-transformers/multi-qa-mpnet-base-dot-v1 model with the cosine function instead of the recommended dot_product. This can be set when initializing the DocumentStore\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "   embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "   model_format=\"sentence_transformers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Union\n",
    "\n",
    "from haystack.nodes import BaseRanker\n",
    "from haystack.schema import Document\n",
    "from haystack.lazy_imports import LazyImport\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "with LazyImport(message=\"Run 'pip install farm-haystack[inference]'\") as torch_and_transformers_import:\n",
    "    import torch\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from haystack.modeling.utils import initialize_device_settings  # pylint: disable=ungrouped-imports\n",
    "\n",
    "\n",
    "class DiversityRanker(BaseRanker):\n",
    "    \"\"\"\n",
    "    Implements a document ranking algorithm that orders documents in such a way as to maximize the overall diversity\n",
    "    of the documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: Union[str, Path] = \"all-MiniLM-L6-v2\",\n",
    "        top_k: Optional[int] = None,\n",
    "        use_gpu: Optional[bool] = True,\n",
    "        devices: Optional[List[Union[str, \"torch.device\"]]] = None,\n",
    "        similarity: Literal[\"dot_product\", \"cosine\"] = \"dot_product\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a DiversityRanker.\n",
    "\n",
    "        :param model_name_or_path: Path to a pretrained sentence-transformers model.\n",
    "        :param top_k: The maximum number of documents to return.\n",
    "        :param use_gpu: Whether to use GPU (if available). If no GPUs are available, it falls back on a CPU.\n",
    "        :param devices: List of torch devices (for example, cuda:0, cpu, mps) to limit inference to specific devices.\n",
    "        :param similarity: Whether to use dot product or cosine similarity. Can be set to \"dot_product\" (default) or \"cosine\".\n",
    "        \"\"\"\n",
    "        torch_and_transformers_import.check()\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.devices, _ = initialize_device_settings(devices=devices, use_cuda=use_gpu, multi_gpu=True)\n",
    "        self.model = SentenceTransformer(model_name_or_path, device=str(self.devices[0]))\n",
    "        self.similarity = similarity\n",
    "\n",
    "    def predict(self, query: str, documents: List[Document], top_k: Optional[int] = None) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Rank the documents based on their diversity and return the top_k documents.\n",
    "\n",
    "        :param query: The query.\n",
    "        :param documents: A list of Document objects that should be ranked.\n",
    "        :param top_k: The maximum number of documents to return.\n",
    "\n",
    "        :return: A list of top_k documents ranked based on diversity.\n",
    "        \"\"\"\n",
    "        if query is None or len(query) == 0:\n",
    "            raise ValueError(\"Query is empty\")\n",
    "        if documents is None or len(documents) == 0:\n",
    "            raise ValueError(\"No documents to choose from\")\n",
    "\n",
    "        top_k = top_k or self.top_k\n",
    "        diversity_sorted = self.greedy_diversity_order(query=query, documents=documents)\n",
    "        return diversity_sorted[:top_k]\n",
    "\n",
    "    def greedy_diversity_order(self, query: str, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Orders the given list of documents to maximize diversity. The algorithm first calculates embeddings for\n",
    "        each document and the query. It starts by selecting the document that is semantically closest to the query.\n",
    "        Then, for each remaining document, it selects the one that, on average, is least similar to the already\n",
    "        selected documents. This process continues until all documents are selected, resulting in a list where\n",
    "        each subsequent document contributes the most to the overall diversity of the selected set.\n",
    "\n",
    "        :param query: The search query.\n",
    "        :param documents: The list of Document objects to be ranked.\n",
    "\n",
    "        :return: A list of documents ordered to maximize diversity.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate embeddings\n",
    "        doc_embeddings: torch.Tensor = self.model.encode([d.content for d in documents], convert_to_tensor=True)\n",
    "        query_embedding: torch.Tensor = self.model.encode([query], convert_to_tensor=True)\n",
    "\n",
    "        if self.similarity == \"dot_product\":\n",
    "            doc_embeddings /= torch.norm(doc_embeddings, p=2, dim=-1).unsqueeze(-1)\n",
    "            query_embedding /= torch.norm(query_embedding, p=2, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        n = len(documents)\n",
    "        selected: List[int] = []\n",
    "\n",
    "        # Compute the similarity vector between the query and documents\n",
    "        query_doc_sim: torch.Tensor = query_embedding @ doc_embeddings.T\n",
    "\n",
    "        # Start with the document with the highest similarity to the query\n",
    "        selected.append(int(torch.argmax(query_doc_sim).item()))\n",
    "\n",
    "        selected_sum = doc_embeddings[selected[0]] / n\n",
    "\n",
    "        while len(selected) < n:\n",
    "            # Compute mean of dot products of all selected documents and all other documents\n",
    "            similarities = selected_sum @ doc_embeddings.T\n",
    "            # Mask documents that are already selected\n",
    "            similarities[selected] = torch.inf\n",
    "            # Select the document with the lowest total similarity score\n",
    "            index_unselected = int(torch.argmin(similarities).item())\n",
    "\n",
    "            selected.append(index_unselected)\n",
    "            # It's enough just to add to the selected vectors because dot product is distributive\n",
    "            # It's divided by n for numerical stability\n",
    "            selected_sum += doc_embeddings[index_unselected] / n\n",
    "\n",
    "        ranked_docs: List[Document] = [documents[i] for i in selected]\n",
    "\n",
    "        return ranked_docs\n",
    "\n",
    "    def predict_batch(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        documents: Union[List[Document], List[List[Document]]],\n",
    "        top_k: Optional[float] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> Union[List[Document], List[List[Document]]]:\n",
    "        \"\"\"\n",
    "        Rank the documents based on their diversity and return the top_k documents.\n",
    "\n",
    "        :param queries: The queries.\n",
    "        :param documents: A list (or a list of lists) of Document objects that should be ranked.\n",
    "        :param top_k: The maximum number of documents to return.\n",
    "        :param batch_size: The number of documents to process in one batch.\n",
    "\n",
    "        :return: A list (or a list of lists) of top_k documents ranked based on diversity.\n",
    "        \"\"\"\n",
    "        if queries is None or len(queries) == 0:\n",
    "            raise ValueError(\"No queries to choose from\")\n",
    "        if documents is None or len(documents) == 0:\n",
    "            raise ValueError(\"No documents to choose from\")\n",
    "        if len(documents) > 0 and isinstance(documents[0], Document):\n",
    "            # Docs case 1: single list of Documents -> rerank single list of Documents based on single query\n",
    "            if len(queries) != 1:\n",
    "                raise ValueError(\"Number of queries must be 1 if a single list of Documents is provided.\")\n",
    "            return self.predict(query=queries[0], documents=documents, top_k=top_k)  # type: ignore\n",
    "        else:\n",
    "            # Docs case 2: list of lists of Documents -> rerank each list of Documents based on corresponding query\n",
    "            # If queries contains a single query, apply it to each list of Documents\n",
    "            if len(queries) == 1:\n",
    "                queries = queries * len(documents)\n",
    "            if len(queries) != len(documents):\n",
    "                raise ValueError(\"Number of queries must be equal to number of provided Document lists.\")\n",
    "\n",
    "            results = []\n",
    "            for query, cur_docs in zip(queries, documents):\n",
    "                results.append(self.predict(query=query, documents=cur_docs, top_k=top_k))  # type: ignore\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import TopPSampler\n",
    "sampler = TopPSampler(top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "p = Pipeline()\n",
    "p.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "p.add_node(component=sampler, name=\"Sampler\", inputs=[\"Retriever\"])\n",
    "p.add_node(component=DiversityRanker(), name=\"DiversityRanker\", inputs=[\"Sampler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.42it/s]\n"
     ]
    }
   ],
   "source": [
    "out = p.run(query = \"Best things to do in\",\n",
    "params = {\n",
    "    \"Retriever\":{\n",
    "        \"top_k\":50,\n",
    "        \"filters\": {\"entity_words\":[\"Bergen\",\"bergen\"]}\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Retrieval pipeline, now add question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "prompt_template = PromptTemplate(\n",
    "    prompt=\"\"\"\n",
    "    Answer the question truthfully based solely on the given documents. If the documents do not contain the answer to the question, say that answering is not possible given the available information. Your answer should be no longer than 50 words.\n",
    "    Documents:{join(documents)}\n",
    "    Question:{query}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"text-davinci-003\", api_key=\"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\", default_prompt_template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.add_node(component=prompt_node, name=\"Prompt_node\", inputs=[\"DiversityRanker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n"
     ]
    }
   ],
   "source": [
    "out = p.run(query = \"Short Bergen history summary:\",\n",
    "params = {\n",
    "    \"Retriever\":{\n",
    "        \"top_k\":50,\n",
    "        \"filters\": {\"entity_words\":[\"Bergen\",\"bergen\"]}\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bergen was founded by the last Vikings in 1070 CE and was a main fishing capital where merchants from the Hanseatic League grew rich off North Sea trade. Bryggen is Bergen's historic district, boasting a Unesco World Heritage–listed waterfront of ramshackle wooden shops and old merchant warehouses. In 2023, the city is launching new flight routes to major European destinations.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"answers\"][0].answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('haystack_stuff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d930c8f90cef31d5c3eafdb0997039099ff2e11e0c9956a78e415db9fc54a201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
