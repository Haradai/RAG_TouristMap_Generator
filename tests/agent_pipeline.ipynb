{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step of what I want the agent to do when processing each place:\n",
    "\n",
    "1. What info do I have about this place?\n",
    "    1st source -> duckduckgo answers (should get a description)\n",
    "    2nd source -> documents qa\n",
    "\n",
    "    A:\n",
    "        What is the main economic motor of this place?\n",
    "            (Use the documents provided to answer, if you are unable because the documents do not contain that information you can use the following tools:)\n",
    "\n",
    "            Available tools:\n",
    "                -Question answering through documents database\n",
    "                -Google search answers. (reality duckduckgo, this name is to not to confuse)\n",
    "                -General knowledge agent, you can ask questions and get answers but should later on fact check.\n",
    "    \n",
    "    B:\n",
    "        1.Is this place primarily a tourist destination? In other words, does its economy mainly depend on tourism?\n",
    "\n",
    "        2.In what categories does this place fall on? Can be more than one at a time.\n",
    "        (nature, sports, shopping, city, cultural, historic, art, nightlife, romantic, food and cuisine)\n",
    "\n",
    "        3. Why does it fall on each of the selected categories?\n",
    "    \n",
    "    C: What is the climate like in this place?\n",
    "    \n",
    "    D: How is the public transport in this place? Is it fast? Does it have frequent delays? Is it cheap?\n",
    "\n",
    "    E: How safe is this place? Should I be careful to not be robbed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to answer one of the following sections at a time and keep the answers in a new document store.\n",
    "\n",
    "The summary of the place will be in the text of each document and then categories and answer to each question in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_haystack.document_stores import QdrantDocumentStore\n",
    "places_store = QdrantDocumentStore(\n",
    "    \":memory:\",\n",
    "    index=\"Document\",\n",
    "    embedding_dim=768,\n",
    "    recreate_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we'll do some tests with a prompt node and agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get api_key\n",
    "with open('OPENAI_API_KEY.txt') as f:\n",
    "    api_key = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mPromptNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPromptModel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'google/flan-t5-base'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdefault_prompt_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_variable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_auth_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "The PromptNode class is the central abstraction in Haystack's large language model (LLM) support. PromptNode\n",
      "supports multiple NLP tasks out of the box. You can use it to perform tasks such as\n",
      "summarization, question answering, question generation, and more, using a single, unified model within the Haystack\n",
      "framework.\n",
      "\n",
      "One of the benefits of PromptNode is that you can use it to define and add additional prompt templates\n",
      " the model supports. Defining additional prompt templates makes it possible to extend the model's capabilities\n",
      "and use it for a broader range of NLP tasks in Haystack. Prompt engineers define templates\n",
      "for each NLP task and register them with PromptNode. The burden of defining templates for each task rests on\n",
      "the prompt engineers, not the users.\n",
      "\n",
      "Using an instance of the PromptModel class, you can create multiple PromptNodes that share the same model, saving\n",
      "the memory and time required to load the model multiple times.\n",
      "\n",
      "PromptNode also supports multiple model invocation layers:\n",
      "- Hugging Face transformers (all text2text-generation models)\n",
      "- OpenAI InstructGPT models\n",
      "- Azure OpenAI InstructGPT models\n",
      "\n",
      "But you're not limited to the models listed above, as you can register\n",
      "additional custom model invocation layers.\n",
      "\n",
      "We recommend using LLMs fine-tuned on a collection of datasets phrased as instructions, otherwise we find that the\n",
      "LLM does not \"follow\" prompt instructions well. The list of instruction-following models increases every month,\n",
      "and the current list includes: Flan, OpenAI InstructGPT, opt-iml, bloomz, and mt0 models.\n",
      "\n",
      "For more details, see [PromptNode](https://docs.haystack.deepset.ai/docs/prompt_node).\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Creates a PromptNode instance.\n",
      "\n",
      ":param model_name_or_path: The name of the model to use or an instance of the PromptModel.\n",
      ":param default_prompt_template: The default prompt template to use for the model.\n",
      ":param output_variable: The name of the output variable in which you want to store the inference results.\n",
      "    If not set, PromptNode uses PromptTemplate's output_variable. If PromptTemplate's output_variable is not set, the default name is `results`.\n",
      ":param max_length: The maximum number of tokens the generated text output can have.\n",
      ":param api_key: The API key to use for the model.\n",
      ":param use_auth_token: The authentication token to use for the model.\n",
      ":param use_gpu: Whether to use GPU or not.\n",
      ":param devices: The devices to use for the model.\n",
      ":param top_k: The number of independently generated texts to return per prompt. For example, if you set top_k=3, the model will generate three answers to the query.\n",
      ":param stop_words: Stops text generation if any of the stop words is generated.\n",
      ":param model_kwargs: Additional keyword arguments passed when loading the model specified in `model_name_or_path`.\n",
      ":param debug: Whether to include the used prompts as debug information in the output under the key _debug.\n",
      "\n",
      "Note that Azure OpenAI InstructGPT models require two additional parameters: azure_base_url (the URL for the\n",
      "Azure OpenAI API endpoint, usually in the form `https://<your-endpoint>.openai.azure.com') and\n",
      "azure_deployment_name (the name of the Azure OpenAI API deployment).\n",
      "You should specify these parameters in the `model_kwargs` dictionary.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniforge3/envs/haystacky/lib/python3.8/site-packages/haystack/nodes/prompt/prompt_node.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "PromptNode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepsmachine/miniforge3/envs/haystacky/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "prompt_node = PromptNode(model_name_or_path=\"gpt-3.5-turbo\", api_key=api_key, stop_words=[\"Observation:\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
